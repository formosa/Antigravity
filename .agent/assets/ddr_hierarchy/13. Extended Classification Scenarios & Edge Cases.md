
## 13. Extended Classification Scenarios & Edge Cases

### 13.1 Multi-Tier Decomposition Patterns

#### Pattern 1: Security Requirement Cascade

**Business Driver (BRD):**

```
|BRD-11|: "Data Sovereignty Compliance"
Ensure all user data remains within jurisdictional boundaries to comply
with GDPR, CCPA, and healthcare data protection regulations. This enables
deployment in regulated industries (healthcare, finance, government).

```

-   **Abstraction:** Regulatory compliance as competitive advantage
-   **Stakeholder:** Legal/Compliance teams, Enterprise customers

**System Constraint (NFR):**

```
|NFR-10| ← |BRD-11|: "Data Locality Enforcement"
  |NFR-10.1|: No network transmission beyond localhost (127.0.0.1).
  |NFR-10.2|: No filesystem access outside designated data directory.
  |NFR-10.3|: All model weights must be locally stored (<5GB total).
  |NFR-10.4|: Encryption at rest: AES-256 for persistent logs/state.

```

-   **Abstraction:** Technical boundaries enforcing business requirement
-   **Measurable:** File paths, encryption standards, size limits

**Functional Specification (FSD):**

```
|FSD-11| ← |NFR-10|: "Secure Data Handling"
  |FSD-11.1|: Core validates all file paths against whitelist before access.
  |FSD-11.2|: LogServer encrypts files before writing to disk.
  |FSD-11.3|: Runtime rejects model load requests outside approved directory.
  |FSD-11.4|: UI displays data locality status indicator (green="local only").

```

-   **Abstraction:** Observable system behaviors enforcing constraints
-   **User-Facing:** Status indicators, error messages

**Architecture (SAD):**

```
|SAD-9| ← |FSD-11.1|: "Path Validation Strategy"
  |SAD-9.1|: Pattern: Whitelist validator with chroot-style restriction.
  |SAD-9.2|: Core maintains allowed_paths registry (loaded from config).
  |SAD-9.3|: All Services query Core before filesystem operations.

```

-   **Abstraction:** Architectural mechanism implementing validation
-   **Technology-Neutral:** Doesn't specify validation library

**Data Contract (ICD):**

```yaml
|ICD-7| ← |SAD-9.2|: "Security Configuration Schema"
security:
  allowed_directories:
    - "./models"
    - "./logs"
    - "./extensions"
  encryption:
    algorithm: "AES-256-CBC"
    key_derivation: "PBKDF2"
    iterations: 100000

```

-   **Abstraction:** Exact configuration structure
-   **Validation:** YAML schema with required fields

**Component Design (TDD):**

```
|TDD-6| ← |ICD-7|, |SAD-9.1|: "Component: PathValidator"
  |TDD-6.1|: Class: PathValidator
  |TDD-6.2|: Dependencies: pathlib, os
  |TDD-6.3|: Method: is_allowed(path: Path) -> bool
  |TDD-6.4|: Internal: _normalize_path() resolves symlinks, checks whitelist
  |TDD-6.5|: Raises: SecurityError if path outside allowed directories

```

-   **Abstraction:** Class structure without implementation logic
-   **Contract:** Method signatures, exceptions

**Code Stub (ISP):**

```python
|ISP-7| ← |TDD-6|:

from pathlib import Path
from typing import List

class PathValidator:
    """
    Validates filesystem paths against whitelist.

    Implements
    ----------
    |TDD-6|, |FSD-11.1|

    Security
    --------
    |NFR-10.2|: Prevents directory traversal attacks

    Attributes
    ----------
    allowed_dirs : List[Path]
        Whitelisted directories from |ICD-7|
    """

    def __init__(self, allowed_dirs: List[str]):
        """
        Initialize validator with allowed directories.

        Parameters
        ----------
        allowed_dirs : List[str]
            Paths from security.allowed_directories (|ICD-7|)

        Implementation Notes
        --------------------
        1. Convert strings to Path objects
        2. Resolve to absolute paths (resolve symlinks)
        3. Store in self.allowed_dirs

        References
        ----------
        |TDD-6.2|, |SAD-9.2|
        """
        pass

    def is_allowed(self, path: Path) -> bool:
        """
        Check if path is within allowed directories.

        Parameters
        ----------
        path : Path
            Path to validate

        Returns
        -------
        bool
            True if path is within whitelist

        Raises
        ------
        SecurityError
            If path attempts directory traversal (|TDD-6.5|)

        Implementation Notes
        --------------------
        1. Resolve path to absolute (handle .., symlinks)
        2. Check if any allowed_dir is parent of path
        3. Use path.is_relative_to() for safety

        References
        ----------
        |TDD-6.3|, |FSD-11.1|
        """
        pass

```

**Traceability Chain:**

```
|ISP-7| ← |TDD-6| ← |ICD-7|, |SAD-9| ← |FSD-11| ← |NFR-10| ← |BRD-11|

```

----------

#### Pattern 2: Performance Optimization Cascade

**Business Goal (BRD):**

```
|BRD-12|: "Real-Time Conversational Experience"
Enable fluid, human-like conversational flow with minimal perceived latency
to increase user engagement and reduce abandonment rates (target: <5% session
abandonment due to lag).

```

**Performance Constraint (NFR):**

```
|NFR-11| ← |BRD-12|: "End-to-End Latency Budget"
  |NFR-11.1|: Voice-to-Response (E2E): ≤3s (95th percentile).
  |NFR-11.2|: Breakdown: STT(500ms) + LLM(1500ms) + TTS(800ms) + IPC(200ms).
  |NFR-11.3|: First-token latency (LLM): ≤200ms.
  |NFR-11.4|: GPU utilization: ≥80% during inference (avoid idle waste).

```

**Feature Specification (FSD):**

```
|FSD-12| ← |NFR-11|: "Progressive Response Rendering"
  |FSD-12.1|: UI displays "thinking" indicator within 100ms of user input.
  |FSD-12.2|: LLM tokens stream to UI as generated (no buffer wait).
  |FSD-12.3|: TTS begins synthesis after first sentence (≥5 tokens).
  |FSD-12.4|: Audio playback starts before full synthesis completes.

```

**Architecture (SAD):**

```
|SAD-10| ← |FSD-12.2|: "Streaming Architecture"
  |SAD-10.1|: Pattern: Producer-Consumer with bounded queues.
  |SAD-10.2|: LLM yields tokens to queue (non-blocking generation).
  |SAD-10.3|: UI consumes tokens via polling (100ms interval).
  |SAD-10.4|: Back-pressure: LLM pauses if queue full (size=50 tokens).

```

**Data Contract (ICD):**

```json
|ICD-8| ← |SAD-10.2|: "Streaming Token Schema"
{
  "type": "token_delta" | "stream_end",
  "request_id": "uuid-v4",
  "sequence_num": 0,  // Monotonic counter for ordering
  "token": "string",  // Single token (or null if stream_end)
  "cumulative_text": "string"  // Full text so far (for UI fallback)
}

```

**Component Design (TDD):**

```
|TDD-7| ← |ICD-8|, |SAD-10.2|: "Component: LLMStreamer"
  |TDD-7.1|: Class: LLMStreamer (runs in Runtime Process)
  |TDD-7.2|: Dependencies: onnxruntime-gpu, queue, threading
  |TDD-7.3|: Method: generate_stream(prompt: str, request_id: str) -> Generator
  |TDD-7.4|: Internal: token_queue (maxsize=50 per SAD-10.4)
  |TDD-7.5|: Sends token_delta messages via ServiceClient for each yield

```

**Code Stub (ISP):**

```python
|ISP-8| ← |TDD-7|:

from typing import Generator
import queue
import threading

class LLMStreamer:
    """
    Streaming text generation with back-pressure control.

    Implements
    ----------
    |TDD-7|, |FSD-12.2|

    Performance
    -----------
    |NFR-11.3|: First token within 200ms
    |NFR-11.4|: Maintains ≥80% GPU utilization

    Attributes
    ----------
    token_queue : queue.Queue
        Bounded queue for back-pressure (|SAD-10.4|)
    """

    def __init__(self, model_session, client: ServiceClient):
        """
        Initialize streamer with ONNX session and IPC client.

        Parameters
        ----------
        model_session : ort.InferenceSession
            Pre-loaded ONNX model
        client : ServiceClient
            For sending token_delta messages

        References
        ----------
        |TDD-7.2|
        """
        self.token_queue = queue.Queue(maxsize=50)  # |SAD-10.4|
        pass

    def generate_stream(self, prompt: str, request_id: str) -> Generator[str, None, None]:
        """
        Generate tokens and yield to consumer.

        Parameters
        ----------
        prompt : str
            User input text
        request_id : str
            UUID for correlation (|ICD-8|)

        Yields
        ------
        str
            Individual tokens

        Implementation Notes
        --------------------
        1. Tokenize prompt, prepare ONNX inputs
        2. For each generation step:
           a. Run ONNX inference (single token)
           b. Decode token to string
           c. Construct token_delta message (|ICD-8|)
           d. Send via self.client.send_request()
           e. Yield token to caller
           f. Check if token_queue full (back-pressure)
        3. Send stream_end message after EOS token

        Performance
        -----------
        - First yield MUST occur within 200ms (|NFR-11.3|)
        - Use ort.SessionOptions.graph_optimization_level = 99

        References
        ----------
        |TDD-7.3|, |SAD-10.2|, |FSD-12.2|
        """
        pass

```

----------

### 13.2 Conflict Resolution Matrices

#### Scenario: Contradictory Requirements

**Conflict:**

```
|NFR-4.3|: "LLM Inference: <1s average response time."
|NFR-10.3|: "All model weights <5GB total (for data sovereignty)."

PROBLEM: Quantized models <5GB achieve only ~1.5s inference time on RTX 3080.

```

**Resolution Framework:**


| Option | BRD Impact | NFR Changes | FSD Changes | SAD Changes | Risk |
| :--- | :--- | :--- | :--- | :--- | :--- |
| **A: Relax Latency** | Acceptable if <2s | Update NFR-4.3 → <2s | No change | No change | Low user satisfaction |
| **B: Increase Model Size** | Violates BRD-11 | Update NFR-10.3 → <8GB | No change | No change | Regulatory risk |
| **C: Upgrade Hardware** | Cost increase | Update NFR-1.2 → RTX 4090 | No change | No change | Budget impact |
| **D: Hybrid Approach** | Partial compliance | Add NFR-11.5: "Fast mode" | Add FSD-13: "User selects mode" | Update SAD for conditional loading | Complexity increase |

**Recommended: Option D (Hybrid)**

**Updated Documentation:**

```
|BRD-11.1| (NEW): "Flexible Compliance Modes"
Support both strict compliance (offline, <5GB) and performance mode
(relaxed limits) to serve different market segments.

|NFR-11.5| ← |BRD-11.1|: "Model Size Modes"
  - Compliance Mode: Models ≤5GB, latency ≤2s
  - Performance Mode: Models ≤12GB, latency ≤1s

|FSD-13| ← |NFR-11.5|: "User Mode Selection"
  |FSD-13.1|: UI provides mode toggle in settings (requires restart).
  |FSD-13.2|: Core validates hardware capabilities before mode switch.
  |FSD-13.3|: Compliance mode disables network features (strict isolation).

|SAD-11| ← |FSD-13|: "Conditional Model Loading"
  |SAD-11.1|: Config schema includes 'operating_mode' field.
  |SAD-11.2|: Runtime selects model manifest based on mode.

```

----------

### 13.3 Cross-Process Feature Mapping

#### Feature: "Conversation History with Semantic Search"

This feature spans all processes and tiers. Map it comprehensively:

**BRD (Business Value):**

```
|BRD-13|: "Contextual Memory"
Enable users to reference past conversations without manual note-taking,
increasing productivity and reducing repetitive queries (target: 30% reduction
in duplicate questions).

```

**NFR (Constraints):**

```
|NFR-12| ← |BRD-13|: "Memory Subsystem Constraints"
  |NFR-12.1|: Embedding model: <500MB, CPU-only (for continuous background indexing).
  |NFR-12.2|: Vector search: <50ms for 10k entries (HNSW index).
  |NFR-12.3|: Storage: SQLite database, <1GB for 1 year of conversations.
  |NFR-12.4|: Privacy: No cloud sync, all data local.

```

**FSD (Capabilities):**

```
|FSD-14| ← |NFR-12|: "Conversation Indexing"
  |FSD-14.1|: After each LLM response, generate embedding (384-dim vector).
  |FSD-14.2|: Store in local vector database (conversation_id, timestamp, text, embedding).
  |FSD-14.3|: User can query: "What did I ask about recipes last week?"
  |FSD-14.4|: System returns top-5 semantic matches with timestamps.
  |FSD-14.5|: Clicking match loads full conversation in UI sidebar.

```

**SAD (Architecture):**

```
|SAD-12| ← |FSD-14|: "Memory Architecture"
  |SAD-12.1|: New Service: MemoryService (CPU-bound, separate process).
  |SAD-12.2|: Pattern: Async indexing (POST request, no blocking wait).
  |SAD-12.3|: Pattern: Sync search (GET request, blocks UI until results).
  |SAD-12.4|: Database: SQLite with FTS5 (full-text) + FAISS (vector).
  |SAD-12.5|: Topology:

    Runtime (LLM) → Core → MemoryService (index)
    UI (search) → Core → MemoryService → UI (results)

```

**ICD (Contracts):**

```json
|ICD-9| ← |SAD-12.2|: "Memory Index Request"
{
  "command": "memory_index",
  "request_id": "uuid",
  "payload": {
    "conversation_id": "uuid",
    "timestamp": "ISO-8601",
    "text": "full conversation text",
    "speaker": "user" | "assistant"
  }
}

|ICD-10| ← |SAD-12.3|: "Memory Search Request"
{
  "command": "memory_search",
  "request_id": "uuid",
  "payload": {
    "query": "recipe for pasta",
    "top_k": 5,
    "date_range": {
      "start": "ISO-8601",
      "end": "ISO-8601"
    }
  }
}

|ICD-11| ← |FSD-14.4|: "Memory Search Response"
{
  "status": "success",
  "results": [
    {
      "conversation_id": "uuid",
      "timestamp": "ISO-8601",
      "snippet": "You asked: 'How do I make carbonara?'",
      "similarity_score": 0.89
    }
  ]
}

```

**TDD (Components):**

```
|TDD-8| ← |ICD-9|, |SAD-12|: "Component: MemoryService"
  |TDD-8.1|: Class: MemoryService (inherits ServiceClient pattern)
  |TDD-8.2|: Dependencies: sentence-transformers, faiss-cpu, sqlite3
  |TDD-8.3|: Method: index_conversation(text: str, metadata: dict)
  |TDD-8.4|: Method: search_memory(query: str, top_k: int) -> List[dict]
  |TDD-8.5|: Internal: embedding_model (all-MiniLM-L6-v2, 384-dim)
  |TDD-8.6|: Internal: faiss_index (HNSW, M=16, efConstruction=200)
  |TDD-8.7|: Internal: sqlite_conn (conversations table + FTS5)

|TDD-9| ← |FSD-14.5|: "Component: UI ConversationSidebar"
  |TDD-9.1|: Class: ConversationSidebar (QWidget)
  |TDD-9.2|: Method: display_results(results: List[dict])
  |TDD-9.3|: Method: on_result_click(conversation_id: str) → load_conversation
  |TDD-9.4|: Signal: conversation_selected(uuid) → connects to main chat view

```

**ISP (Stubs):**

```python
|ISP-9| ← |TDD-8|:

import faiss
import sqlite3
from sentence_transformers import SentenceTransformer
from typing import List, Dict

class MemoryService(ServiceClient):
    """
    Conversation indexing and semantic search service.

    Implements
    ----------
    |TDD-8|, |FSD-14|

    Performance
    -----------
    |NFR-12.2|: Search latency <50ms for 10k entries

    Attributes
    ----------
    embedding_model : SentenceTransformer
        all-MiniLM-L6-v2 (384-dim, |TDD-8.5|)
    faiss_index : faiss.IndexHNSWFlat
        Vector index (|TDD-8.6|)
    db_conn : sqlite3.Connection
        Local conversation database (|TDD-8.7|)
    """

    def __init__(self, config_path: str, db_path: str):
        """
        Initialize memory service with embedding model and database.

        Parameters
        ----------
        config_path : str
            Path to ipc_config.yaml
        db_path : str
            Path to conversations.db (SQLite)

        Implementation Notes
        --------------------
        1. Call super().__init__("memory", config_path)
        2. Load embedding model (sentence-transformers)
        3. Initialize FAISS index or load from disk
        4. Connect to SQLite database
        5. Create tables if not exist (conversations, embeddings)

        References
        ----------
        |TDD-8.2|, |SAD-12.4|
        """
        super().__init__("memory", config_path)
        pass

    def index_conversation(self, text: str, metadata: dict) -> None:
        """
        Generate embedding and store in vector database.

        Parameters
        ----------
        text : str
            Full conversation text
        metadata : dict
            Contains conversation_id, timestamp, speaker (|ICD-9|)

        Implementation Notes
        --------------------
        1. Generate embedding: self.embedding_model.encode(text)
        2. Add to FAISS index: self.faiss_index.add(embedding)
        3. Insert into SQLite: (conversation_id, text, timestamp)
        4. Commit transaction
        5. Send log confirmation (non-blocking)

        Performance
        -----------
        - CPU-only embedding generation (|NFR-12.1|)
        - Async execution (no blocking Core) (|SAD-12.2|)

        References
        ----------
        |TDD-8.3|, |FSD-14.1|
        """
        pass

    def search_memory(self, query: str, top_k: int = 5) -> List[dict]:
        """
        Semantic search across conversation history.

        Parameters
        ----------
        query : str
            User search query
        top_k : int
            Number of results to return

        Returns
        -------
        List[dict]
            Results matching |ICD-11| schema

        Implementation Notes
        --------------------
        1. Generate query embedding
        2. FAISS search: distances, indices = index.search(embedding, top_k)
        3. Retrieve metadata from SQLite using indices
        4. Construct response per |ICD-11|
        5. Return results (sorted by similarity_score DESC)

        Performance
        -----------
        - Must complete <50ms for 10k entries (|NFR-12.2|)
        - Use HNSW parameters: M=16, efSearch=64

        References
        ----------
        |TDD-8.4|, |FSD-14.4|
        """
        pass

```

----------
