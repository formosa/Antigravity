
## 24. Extended Example: Complete Multi-Tier Feature

### 24.1 Feature: "Offline Speech Translation"

This extended example demonstrates complete documentation from business case through implementation for a complex feature spanning all tiers.

#### Tier 1: Business Requirements (BRD)

```
|BRD-17|: "Multilingual Accessibility"
Enable real-time speech-to-speech translation to expand market reach into
non-English speaking regions. Target markets: Spanish (LATAM), Mandarin (China),
French (Europe).

**Market Opportunity:**
- 75% of global AI assistant market is non-English speaking
- Current offline translation solutions require cloud connectivity
- Privacy-conscious users in regulated industries (healthcare, legal)
  cannot use cloud-based translation

**Business Objectives:**
- Capture 15% of Spanish-speaking market within 12 months
- Enable deployment in data-sovereignty regions (EU GDPR, China)
- Differentiate from cloud-dependent competitors (Alexa, Google)

**Success Metrics:**
- Translation accuracy: BLEU score ≥30 (conversational quality)
- User adoption: 25% of installations enable translation feature
- Retention: Users continue using translation after 1 month (≥70%)
- Performance: End-to-end translation latency ≤5s (95th percentile)

```

#### Tier 2: Non-Functional Requirements (NFR)

```
|NFR-18| ← |BRD-17|: "Translation System Constraints"
  |NFR-18.1|: Language support (MVP): English ↔ Spanish, English ↔ Mandarin, English ↔ French.
  |NFR-18.2|: Translation model size: ≤2GB per language pair (6GB total for 3 pairs).
  |NFR-18.3|: Translation latency: ≤1.5s for 100-token input.
  |NFR-18.4|: STT accuracy: WER (Word Error Rate) <15% for supported languages.
  |NFR-18.5|: TTS quality: MOS (Mean Opinion Score) ≥4.0 for synthesized speech.
  |NFR-18.6|: Hardware requirement: RTX 3080 sufficient for all models loaded simultaneously.
  |NFR-18.7|: Offline operation: Zero internet dependency after initial model download.
  |NFR-18.8|: Memory footprint: Additional 8GB system RAM for translation pipeline.

**Rationale (NFR-18.2):** 2GB limit per pair allows 3 language pairs within
10GB VRAM budget (4GB reserved for LLM, leaving 6GB for translation).

```

#### Tier 3: Feature Specification Document (FSD)

```
|FSD-20| ← |NFR-18|: "Speech Translation Workflow"
  |FSD-20.1|: User selects source language (Settings → Translation → Source Language).
  |FSD-20.2|: User speaks in source language → System detects language via LID (Language ID).
  |FSD-20.3|: Audio transcribed to source language text (STT in source language).
  |FSD-20.4|: Source text translated to English (if not already English).
  |FSD-20.5|: Translation passed to LLM for conversational response generation.
  |FSD-20.6|: LLM response translated back to source language.
  |FSD-20.7|: Translated response synthesized to speech (TTS in source language).
  |FSD-20.8|: UI displays both source and target text side-by-side for verification.
  |FSD-20.9|: Error handling: If translation confidence <0.7 → "I didn't understand, please repeat."

**User Experience Flow:**
  |FSD-20.10|: Auto-detection mode: System infers language from speech without explicit selection.
  |FSD-20.11|: Mixed-language conversations: User can switch languages mid-conversation.
  |FSD-20.12|: Translation history: Previous translations accessible in conversation sidebar.

**Quality Indicators:**
  |FSD-20.13|: UI shows translation confidence badge (Green ≥0.9, Yellow 0.7-0.9, Red <0.7).
  |FSD-20.14|: User can flag incorrect translations → stored for offline model improvement.

```

#### Tier 4: System Architecture Document (SAD)

```
|SAD-16| ← |FSD-20|: "Translation Pipeline Architecture"
  |SAD-16.1|: Pattern: Sequential pipeline with confidence gating at each stage.
  |SAD-16.2|: Component allocation:
    - LID (Language ID): Audio Process (CPU-based, <50ms)
    - STT (multilingual): Runtime Process (GPU, Whisper multilingual model)
    - Translation: Runtime Process (GPU, NLLB-200 distilled model)
    - TTS (multilingual): Runtime Process (GPU, XTTS-v2 or Kokoro multilingual)
  |SAD-16.3|: Model management: Lazy loading (only load active language pair models).
  |SAD-16.4|: Confidence thresholding: Each stage reports confidence, pipeline aborts if <0.7.
  |SAD-16.5|: Topology:

    Audio (LID) → Core → Runtime (STT source_lang)
                    ↓
                 Runtime (Translate source→en)
                    ↓
                 Core → Runtime (LLM generate response)
                    ↓
                 Runtime (Translate en→source)
                    ↓
                 Runtime (TTS source_lang)
                    ↓
                 Core → UI (display + Audio playback)

**Design Decisions:**
  |SAD-16.1.R1|: Sequential (not parallel) pipeline chosen because each stage
  depends on previous output. Confidence gating prevents cascading errors.

  |SAD-16.2.R1|: All GPU models in Runtime Process to avoid VRAM fragmentation.
  LID in Audio Process because it's CPU-based and time-critical (must run
  immediately after wake word).

  |SAD-16.3.R1|: Lazy loading reduces idle VRAM usage. Spanish user doesn't
  need French models loaded (saves 2GB VRAM for larger LLM context).

```

#### Tier 5: Interface Control Document (ICD)

```json
|ICD-21| ← |SAD-16.2|: "Language Identification Request"
{
  "command": "identify_language",
  "request_id": "uuid",
  "payload": {
    "audio_embedding": [0.12, -0.34, ...],  // Language-specific acoustic features
    "duration_sec": 3.5
  }
}

|ICD-22| ← |ICD-21|: "Language Identification Response"
{
  "request_id": "uuid",
  "status": "success",
  "payload": {
    "detected_language": "es",  // ISO 639-1 code
    "confidence": 0.92,
      {"language": "pt", "confidence": 0.65},
      {"language": "ca", "confidence": 0.23}
    ]
  }
}

|ICD-23| ← |SAD-16.2|: "Translation Request"
{
  "command": "translate_text",
  "request_id": "uuid",
  "payload": {
    "text": "¿Cómo está el clima hoy?",
    "source_lang": "es",
    "target_lang": "en"
  }
}

|ICD-24| ← |FSD-20.13|: "Translation Response"
{
  "request_id": "uuid",
  "status": "success",
  "payload": {
    "translated_text": "How is the weather today?",
    "confidence": 0.89,
    "model_version": "nllb-200-distilled-600M",
    "alternatives": [  // Optional: N-best translations
      "What is the weather like today?",
      "How's the weather today?"
    ]
  }
}

|ICD-25| ← |FSD-20.1|: "User Language Preference Schema (Config)"
# File: user_profiles/{user_id}/preferences.yaml
language:
  primary: "es"           # User's native language
  translation_mode: "auto"  # "auto" | "manual" | "off"
  auto_detect: true       # Use LID vs. rely on primary setting
  tts_voice: "es-female-1"  # Voice preference per language
  display_both: true      # Show source + translation in UI

```

#### Tier 6: Technical Design Document (TDD)

```
|TDD-15| ← |ICD-21|, |SAD-16.2|: "Component: LanguageIdentifier"
  |TDD-15.1|: Class: LanguageIdentifier (runs in Audio Process)
  |TDD-15.2|: Dependencies: speechbrain, torchaudio (CPU-only)
  |TDD-15.3|: Method: identify(audio: np.ndarray) -> LanguageResult
  |TDD-15.4|: Internal: lid_model (SpeechBrain Language ID, <100MB)
  |TDD-15.5|: Returns: LanguageResult dataclass with detected_language, confidence
  |TDD-15.6|: Performance: Must complete <50ms (|NFR-18.3| time budget)

|TDD-16| ← |ICD-23|, |SAD-16.2|: "Component: TranslationService"
  |TDD-16.1|: Class: TranslationService (runs in Runtime Process)
  |TDD-16.2|: Dependencies: onnxruntime-gpu, sentencepiece
  |TDD-16.3|: Method: translate(text: str, src: str, tgt: str) -> TranslationResult
  |TDD-16.4|: Internal: nllb_model (NLLB-200-distilled, 2GB per direction)
  |TDD-16.5|: Internal: model_cache (Dict[Tuple[str,str], ort.InferenceSession])
  |TDD-16.6|: Method: load_model_pair(src: str, tgt: str) -> None (lazy loading)
  |TDD-16.7|: Performance: Batching support for multi-sentence translation

|TDD-17| ← |FSD-20.8|: "Component: UI TranslationDisplay"
  |TDD-17.1|: Class: TranslationDisplay (QWidget)
  |TDD-17.2|: Method: show_translation(source_text: str, translated_text: str, confidence: float)
  |TDD-17.3|: Visual: Two-column layout (Source | Translation)
  |TDD-17.4|: Visual: Confidence badge color-coded per |FSD-20.13|
  |TDD-17.5|: Interaction: Click badge → expand to show alternatives
  |TDD-17.6|: Interaction: Right-click text → "Flag incorrect translation"

```

#### Tier 7: Implementation Stub Prompts (ISP)

```python
|ISP-12| ← |TDD-15|:

import numpy as np
from dataclasses import dataclass
from typing import List, Tuple

@dataclass
class LanguageResult:
    """
    Language identification result.

    Attributes
    ----------
    detected_language : str
        ISO 639-1 language code
    confidence : float
        Detection confidence (0.0-1.0)
    alternatives : List[Tuple[str, float]]
        Alternative predictions (language, confidence)
    """
    detected_language: str
    confidence: float
    alternatives: List[Tuple[str, float]]

class LanguageIdentifier:
    """
    CPU-based language identification from audio.

    Implements
    ----------
    |TDD-15|, |FSD-20.2|

    Performance
    -----------
    |NFR-18.3|: <50ms latency

    Attributes
    ----------
    lid_model : speechbrain.pretrained.EncoderClassifier
        SpeechBrain Language ID model
    supported_languages : List[str]
        ['en', 'es', 'zh', 'fr'] per |NFR-18.1|
    """

    def __init__(self, model_path: str):
        """
        Load language identification model.

        Parameters
        ----------
        model_path : str
            Path to pretrained SpeechBrain model (<100MB)

        Implementation Notes
        --------------------
        1. Load model: speechbrain.pretrained.EncoderClassifier.from_hparams(model_path)
        2. Verify model supports required languages (|NFR-18.1|)
        3. Set device to CPU (Audio Process constraint |NFR-1.1|)

        References
        ----------
        |TDD-15.2|, |TDD-15.4|
        """
        pass

    def identify(self, audio: np.ndarray, sample_rate: int = 16000) -> LanguageResult:
        """
        Identify language from audio sample.

        Parameters
        ----------
        audio : np.ndarray
            Audio waveform (mono, 16kHz recommended)
        sample_rate : int
            Audio sample rate (Hz)

        Returns
        -------
        LanguageResult
            Detection result with confidence scores (|ICD-22|)

        Implementation Notes
        --------------------
        1. Resample audio to model's expected rate if needed
        2. Extract features: embeddings = self.lid_model.encode_batch(audio)
        3. Classify: predictions = self.lid_model.classify_batch(embeddings)
        4. Get top-3 predictions with confidence scores
        5. Construct LanguageResult dataclass
        6. Return result

        Performance
        -----------
        - Must complete <50ms (|NFR-18.3|)
        - Use torch.no_grad() to avoid gradient computation
        - Consider caching embeddings if same audio processed multiple times

        Error Handling
        --------------
        - If confidence <0.5 for all languages: return "unknown"
        - If audio too short (<1s): return error result

        References
        ----------
        |TDD-15.3|, |ICD-21|, |FSD-20.2|
        """
        pass


|ISP-13| ← |TDD-16|:

import onnxruntime as ort
from dataclasses import dataclass
from typing import Dict, Tuple, Optional, List

@dataclass
class TranslationResult:
    """
    Translation output with metadata.

    Attributes
    ----------
    translated_text : str
        Target language translation
    confidence : float
        Translation confidence score
    alternatives : List[str]
        N-best alternative translations
    """
    translated_text: str
    confidence: float
    alternatives: List[str]

class TranslationService:
    """
    Neural machine translation using NLLB-200.

    Implements
    ----------
    |TDD-16|, |FSD-20.4|, |FSD-20.6|

    Constraints
    -----------
    |NFR-18.2|: 2GB per language pair
    |NFR-18.3|: <1.5s for 100 tokens

    Attributes
    ----------
    model_cache : Dict[Tuple[str, str], ort.InferenceSession]
        Loaded ONNX models keyed by (source, target) language pair
    tokenizer : sentencepiece.SentencePieceProcessor
        Shared tokenizer for all language pairs
    """

    def __init__(self, models_dir: str):
        """
        Initialize translation service with model directory.

        Parameters
        ----------
        models_dir : str
            Directory containing NLLB ONNX models
            Expected structure:
            models_dir/
              nllb_en_es.onnx
              nllb_es_en.onnx
              nllb_en_zh.onnx
              ...
              tokenizer.model

        Implementation Notes
        --------------------
        1. Load shared tokenizer (sentencepiece)
        2. Initialize empty model_cache dict
        3. Set ONNX execution providers: ['CUDAExecutionProvider', 'CPUExecutionProvider']
        4. Do NOT preload models (lazy loading per |SAD-16.3|)

        References
        ----------
        |TDD-16.2|, |TDD-16.5|, |SAD-16.3.R1|
        """
        self.model_cache = {}
        pass

    def load_model_pair(self, source_lang: str, target_lang: str) -> None:
        """
        Lazy-load translation model for language pair.

        Parameters
        ----------
        source_lang : str
            ISO 639-1 source language code
        target_lang : str
            ISO 639-1 target language code

        Implementation Notes
        --------------------
        1. Check if (source_lang, target_lang) already in cache → return early
        2. Construct model filename: f"nllb_{source_lang}_{target_lang}.onnx"
        3. Load ONNX session: ort.InferenceSession(model_path, providers=[...])
        4. Store in cache: self.model_cache[(source_lang, target_lang)] = session
        5. Log model load event with VRAM usage

        Performance
        -----------
        - Model loading ~2-3s acceptable (one-time cost)
        - Monitor VRAM: should not exceed 2GB per model (|NFR-18.2|)

        Error Handling
        --------------
        - Raise FileNotFoundError if model file missing
        - Raise RuntimeError if VRAM allocation fails

        References
        ----------
        |TDD-16.6|, |SAD-16.3|
        """
        pass

    def translate(self, text: str, source_lang: str, target_lang: str,
                  num_alternatives: int = 0) -> TranslationResult:
        """
        Translate text from source to target language.

        Parameters
        ----------
        text : str
            Source language text to translate
        source_lang : str
            ISO 639-1 source code
        target_lang : str
            ISO 639-1 target code
        num_alternatives : int
            Number of alternative translations to generate (0-5)

        Returns
        -------
        TranslationResult
            Translation with confidence and alternatives (|ICD-24|)

        Implementation Notes
        --------------------
        1. Ensure model loaded: self.load_model_pair(source_lang, target_lang)
        2. Tokenize input: input_ids = self.tokenizer.encode(text)
        3. Prepare ONNX inputs: {"input_ids": input_ids, "attention_mask": [...]}
        4. Run inference: outputs = session.run(None, inputs)
        5. Decode output: translated_text = self.tokenizer.decode(outputs[0])
        6. Calculate confidence from logits (softmax of top prediction)
        7. If num_alternatives > 0: beam search for N-best list
        8. Construct TranslationResult
        9. Return result

        Performance
        -----------
        - Must complete <1.5s for 100 tokens (|NFR-18.3|)
        - Use batch_size=1 for single translation
        - Consider token-level caching for repeated phrases

        Quality Assurance
        -----------------
        - If confidence <0.7: Set flag in result (|FSD-20.9|)
        - Log translations with confidence <0.5 for review

        References
        ----------
        |TDD-16.3|, |ICD-23|, |ICD-24|, |FSD-20.4|
        """
        pass

```

----------
