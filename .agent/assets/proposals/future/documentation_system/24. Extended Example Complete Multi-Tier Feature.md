
## 24. Extended Example: Complete Multi-Tier Feature

### 24.1 Feature: "Offline Speech Translation"

This extended example demonstrates complete documentation from business case through implementation for a complex feature spanning all tiers.

#### Tier 1: Business Requirements (BRD)
> **Embedded Example Type:** reStructuredText BRD directive
~~~rst
.. brd:: Multilingual Accessibility
   :id: BRD-17

   Enable real-time speech-to-speech translation to expand market reach into non-English speaking regions. Target markets: Spanish (LATAM), Mandarin (China), French (Europe).

   MARKET OPPORTUNITY:
   - 75% of global AI assistant market is non-English speaking
   - Current offline translation solutions require cloud connectivity
   - Privacy-conscious users in regulated industries (healthcare, legal) cannot use cloud-based translation

   BUSINESS OBJECTIVES:
   - Capture 15% of Spanish-speaking market within 12 months
   - Enable deployment in data-sovereignty regions (EU GDPR, China)
   - Differentiate from cloud-dependent competitors (Alexa, Google)

   SUCCESS METRICS:
   - Translation accuracy: BLEU score ≥30 (conversational quality)
   - User adoption: 25% of installations enable translation feature
   - Retention: Users continue using translation after 1 month (≥70%)
   - Performance: End-to-end translation latency ≤5s (95th percentile)
~~~

#### Tier 2: Non-Functional Requirements (NFR)
> **Embedded Example Type:** reStructuredText NFR directive
~~~rst
.. nfr:: Translation System Constraints
   :id: NFR-18
   :links: BRD-17

.. nfr:: Language support (MVP): English ↔ Spanish, English ↔ Mandarin, English ↔ French.
   :id: NFR-18.1
   :links: NFR-18

.. nfr:: Translation model size: ≤2GB per language pair (6GB total for 3 pairs).
   :id: NFR-18.2
   :links: NFR-18

   RATIONALE: 2GB limit per pair allows 3 language pairs within 10GB VRAM budget (4GB reserved for LLM, leaving 6GB for translation).

.. nfr:: Translation latency: ≤1.5s for 100-token input.
   :id: NFR-18.3
   :links: NFR-18

.. nfr:: STT accuracy: WER (Word Error Rate) <15% for supported languages.
   :id: NFR-18.4
   :links: NFR-18

.. nfr:: TTS quality: MOS (Mean Opinion Score) ≥4.0 for synthesized speech.
   :id: NFR-18.5
   :links: NFR-18

.. nfr:: Hardware requirement: RTX 3080 sufficient for all models loaded simultaneously.
   :id: NFR-18.6
   :links: NFR-18

.. nfr:: Offline operation: Zero internet dependency after initial model download.
   :id: NFR-18.7
   :links: NFR-18

.. nfr:: Memory footprint: Additional 8GB system RAM for translation pipeline.
   :id: NFR-18.8
   :links: NFR-18
~~~

#### Tier 3: Feature Specification Document (FSD)
> **Embedded Example Type:** reStructuredText FSD directive
~~~rst
.. fsd:: Speech Translation Workflow
   :id: FSD-20
   :links: NFR-18

.. fsd:: User selects source language (Settings → Translation → Source Language).
   :id: FSD-20.1
   :links: FSD-20

.. fsd:: User speaks in source language → System detects language via LID (Language ID).
   :id: FSD-20.2
   :links: FSD-20

.. fsd:: Audio transcribed to source language text (STT in source language).
   :id: FSD-20.3
   :links: FSD-20

.. fsd:: Source text translated to English (if not already English).
   :id: FSD-20.4
   :links: FSD-20

.. fsd:: Translation passed to LLM for conversational response generation.
   :id: FSD-20.5
   :links: FSD-20

.. fsd:: LLM response translated back to source language.
   :id: FSD-20.6
   :links: FSD-20

.. fsd:: Translated response synthesized to speech (TTS in source language).
   :id: FSD-20.7
   :links: FSD-20

.. fsd:: UI displays both source and target text side-by-side for verification.
   :id: FSD-20.8
   :links: FSD-20

.. fsd:: Error handling: If translation confidence <0.7 → "I didn't understand, please repeat."
   :id: FSD-20.9
   :links: FSD-20

.. fsd:: Auto-detection mode: System infers language from speech without explicit selection.
   :id: FSD-20.10
   :links: FSD-20

.. fsd:: Mixed-language conversations: User can switch languages mid-conversation.
   :id: FSD-20.11
   :links: FSD-20

.. fsd:: Translation history: Previous translations accessible in conversation sidebar.
   :id: FSD-20.12
   :links: FSD-20

.. fsd:: UI shows translation confidence badge (Green ≥0.9, Yellow 0.7-0.9, Red <0.7).
   :id: FSD-20.13
   :links: FSD-20

.. fsd:: User can flag incorrect translations → stored for offline model improvement.
   :id: FSD-20.14
   :links: FSD-20
~~~

#### Tier 4: System Architecture Document (SAD)
> **Embedded Example Type:** reStructuredText SAD directive
~~~rst
.. sad:: Translation Pipeline Architecture
   :id: SAD-16
   :links: FSD-20

.. sad:: Pattern: Sequential pipeline with confidence gating at each stage.
   :id: SAD-16.1
   :links: SAD-16

   RATIONALE: Sequential pipeline chosen because each stage depends on previous output. Confidence gating prevents cascading errors.

.. sad:: Component allocation: LID (Audio Process); STT, Translation, TTS (Runtime Process).
   :id: SAD-16.2
   :links: SAD-16

   RATIONALE: All GPU models in Runtime Process to avoid VRAM fragmentation. LID in Audio Process because it's CPU-based and time-critical.

.. sad:: Model management: Lazy loading (only load active language pair models).
   :id: SAD-16.3
   :links: SAD-16

   RATIONALE: Lazy loading reduces idle VRAM usage.

.. sad:: Confidence thresholding: Each stage reports confidence, pipeline aborts if <0.7.
   :id: SAD-16.4
   :links: SAD-16

.. sad:: Topology
   :id: SAD-16.5
   :links: SAD-16

   Audio (LID) → Core → Runtime (STT source_lang)
                   ↓
                Runtime (Translate source→en)
                   ↓
                Core → Runtime (LLM generate response)
                   ↓
                Runtime (Translate en→source)
                   ↓
                Runtime (TTS source_lang)
                   ↓
                Core → UI (display + Audio playback)
~~~

#### Tier 5: Interface Control Document (ICD)
> **Embedded Example Type:** reStructuredText ICD directive
~~~rst
.. icd:: Language Identification Request
   :id: ICD-21
   :links: SAD-16.2

   .. code-block:: json

      {
        "command": "identify_language",
        "request_id": "uuid",
        "payload": {
          "audio_embedding": [0.12, -0.34, ...],
          "duration_sec": 3.5
        }
      }

.. icd:: Language Identification Response
   :id: ICD-22
   :links: ICD-21

   .. code-block:: json

      {
        "request_id": "uuid",
        "status": "success",
        "payload": {
          "detected_language": "es",
          "confidence": 0.92,
          "alternatives": [
            {"language": "pt", "confidence": 0.65},
            {"language": "ca", "confidence": 0.23}
          ]
        }
      }

.. icd:: Translation Request
   :id: ICD-23
   :links: SAD-16.2

   .. code-block:: json

      {
        "command": "translate_text",
        "request_id": "uuid",
        "payload": {
          "text": "¿Cómo está el clima hoy?",
          "source_lang": "es",
          "target_lang": "en"
        }
      }

.. icd:: Translation Response
   :id: ICD-24
   :links: FSD-20.13

   .. code-block:: json

      {
        "request_id": "uuid",
        "status": "success",
        "payload": {
          "translated_text": "How is the weather today?",
          "confidence": 0.89,
          "model_version": "nllb-200-distilled-600M",
          "alternatives": [
            "What is the weather like today?",
            "How's the weather today?"
          ]
        }
      }

.. icd:: User Language Preference Schema (Config)
   :id: ICD-25
   :links: FSD-20.1

   .. code-block:: yaml

      # user_profiles/{user_id}/preferences.yaml
      language:
        primary: "es"
        translation_mode: "auto"
        auto_detect: true
        tts_voice: "es-female-1"
        display_both: true
~~~

#### Tier 6: Technical Design Document (TDD)
> **Embedded Example Type:** reStructuredText TDD directive
~~~rst
.. tdd:: Component: LanguageIdentifier
   :id: TDD-15
   :links: ICD-21, SAD-16.2

.. tdd:: Class: LanguageIdentifier (runs in Audio Process)
   :id: TDD-15.1
   :links: TDD-15

.. tdd:: Dependencies: speechbrain, torchaudio (CPU-only)
   :id: TDD-15.2
   :links: TDD-15

.. tdd:: Method: identify(audio: np.ndarray) -> LanguageResult
   :id: TDD-15.3
   :links: TDD-15

.. tdd:: Internal: lid_model (SpeechBrain Language ID, <100MB)
   :id: TDD-15.4
   :links: TDD-15

.. tdd:: Returns: LanguageResult dataclass
   :id: TDD-15.5
   :links: TDD-15

.. tdd:: Performance: Must complete <50ms.
   :id: TDD-15.6
   :links: TDD-15

.. tdd:: Component: TranslationService
   :id: TDD-16
   :links: ICD-23, SAD-16.2

.. tdd:: Class: TranslationService (runs in Runtime Process)
   :id: TDD-16.1
   :links: TDD-16

.. tdd:: Dependencies: onnxruntime-gpu, sentencepiece
   :id: TDD-16.2
   :links: TDD-16

.. tdd:: Method: translate(text: str, src: str, tgt: str) -> TranslationResult
   :id: TDD-16.3
   :links: TDD-16

.. tdd:: Internal: nllb_model
   :id: TDD-16.4
   :links: TDD-16

.. tdd:: Internal: model_cache
   :id: TDD-16.5
   :links: TDD-16

.. tdd:: Method: load_model_pair(src: str, tgt: str) -> None
   :id: TDD-16.6
   :links: TDD-16

.. tdd:: Performance: Batching support
   :id: TDD-16.7
   :links: TDD-16

.. tdd:: Component: UI TranslationDisplay
   :id: TDD-17
   :links: FSD-20.8

.. tdd:: Class: TranslationDisplay (QWidget)
   :id: TDD-17.1
   :links: TDD-17

.. tdd:: Method: show_translation(source_text: str, trans_text: str, conf: float)
   :id: TDD-17.2
   :links: TDD-17

.. tdd:: Visual: Two-column layout (Source | Translation)
   :id: TDD-17.3
   :links: TDD-17

.. tdd:: Visual: Confidence badge color-coded per FSD-20.13
   :id: TDD-17.4
   :links: TDD-17

.. tdd:: Interaction: Click badge → expand; Right-click → flag.
   :id: TDD-17.5
   :links: TDD-17
~~~

#### Tier 7: Implementation Stub Prompts (ISP)
> **Embedded Example Code:** Python class/stub
~~~python
# |ISP-12|: "Language Identifier Implementation Stub"

import numpy as np
from dataclasses import dataclass
from typing import List, Tuple

@dataclass
class LanguageResult:
    """
    Language identification result.

    Attributes
    ----------
    detected_language : str
        ISO 639-1 language code
    confidence : float
        Detection confidence (0.0-1.0)
    alternatives : List[Tuple[str, float]]
        Alternative predictions (language, confidence)
    """
    detected_language: str
    confidence: float
    alternatives: List[Tuple[str, float]]

class LanguageIdentifier:
    """
    CPU-based language identification from audio.

    Implements
    ----------
    |TDD-15|, |FSD-20.2|

    Performance
    -----------
    |NFR-18.3|: <50ms latency

    Attributes
    ----------
    lid_model : speechbrain.pretrained.EncoderClassifier
        SpeechBrain Language ID model
    supported_languages : List[str]
        ['en', 'es', 'zh', 'fr'] per |NFR-18.1|
    """

    def __init__(self, model_path: str):
        """
        Load language identification model.

        Parameters
        ----------
        model_path : str
            Path to pretrained SpeechBrain model (<100MB)

        Implementation Notes
        --------------------
        1. Load model: speechbrain.pretrained.EncoderClassifier.from_hparams(model_path)
        2. Verify model supports required languages (|NFR-18.1|)
        3. Set device to CPU (Audio Process constraint |NFR-1.1|)

        References
        ----------
        |TDD-15.2|, |TDD-15.4|
        """
        pass

    def identify(self, audio: np.ndarray, sample_rate: int = 16000) -> LanguageResult:
        """
        Identify language from audio sample.

        Parameters
        ----------
        audio : np.ndarray
            Audio waveform (mono, 16kHz recommended)
        sample_rate : int
            Audio sample rate (Hz)

        Returns
        -------
        LanguageResult
            Detection result with confidence scores (|ICD-22|)

        Implementation Notes
        --------------------
        1. Resample audio to model's expected rate if needed
        2. Extract features: embeddings = self.lid_model.encode_batch(audio)
        3. Classify: predictions = self.lid_model.classify_batch(embeddings)
        4. Get top-3 predictions with confidence scores
        5. Construct LanguageResult dataclass
        6. Return result

        Performance
        -----------
        - Must complete <50ms (|NFR-18.3|)
        - Use torch.no_grad() to avoid gradient computation
        - Consider caching embeddings if same audio processed multiple times

        Error Handling
        --------------
        - If confidence <0.5 for all languages: return "unknown"
        - If audio too short (<1s): return error result

        References
        ----------
        |TDD-15.3|, |ICD-21|, |FSD-20.2|
        """
        pass


# |ISP-13|: "Translation Service Implementation Stub"

import onnxruntime as ort
from dataclasses import dataclass
from typing import Dict, Tuple, Optional, List

@dataclass
class TranslationResult:
    """
    Translation output with metadata.

    Attributes
    ----------
    translated_text : str
        Target language translation
    confidence : float
        Translation confidence score
    alternatives : List[str]
        N-best alternative translations
    """
    translated_text: str
    confidence: float
    alternatives: List[str]

class TranslationService:
    """
    Neural machine translation using NLLB-200.

    Implements
    ----------
    |TDD-16|, |FSD-20.4|, |FSD-20.6|

    Constraints
    -----------
    |NFR-18.2|: 2GB per language pair
    |NFR-18.3|: <1.5s for 100 tokens

    Attributes
    ----------
    model_cache : Dict[Tuple[str, str], ort.InferenceSession]
        Loaded ONNX models keyed by (source, target) language pair
    tokenizer : sentencepiece.SentencePieceProcessor
        Shared tokenizer for all language pairs
    """

    def __init__(self, models_dir: str):
        """
        Initialize translation service with model directory.

        Parameters
        ----------
        models_dir : str
            Directory containing NLLB ONNX models
            Expected structure:
            models_dir/
              nllb_en_es.onnx
              nllb_es_en.onnx
              nllb_en_zh.onnx
              ...
              tokenizer.model

        Implementation Notes
        --------------------
        1. Load shared tokenizer (sentencepiece)
        2. Initialize empty model_cache dict
        3. Set ONNX execution providers: ['CUDAExecutionProvider', 'CPUExecutionProvider']
        4. Do NOT preload models (lazy loading per |SAD-16.3|)

        References
        ----------
        |TDD-16.2|, |TDD-16.5|, |SAD-16.3.R1|
        """
        self.model_cache = {}
        pass

    def load_model_pair(self, source_lang: str, target_lang: str) -> None:
        """
        Lazy-load translation model for language pair.

        Parameters
        ----------
        source_lang : str
            ISO 639-1 source language code
        target_lang : str
            ISO 639-1 target language code

        Implementation Notes
        --------------------
        1. Check if (source_lang, target_lang) already in cache → return early
        2. Construct model filename: f"nllb_{source_lang}_{target_lang}.onnx"
        3. Load ONNX session: ort.InferenceSession(model_path, providers=[...])
        4. Store in cache: self.model_cache[(source_lang, target_lang)] = session
        5. Log model load event with VRAM usage

        Performance
        -----------
        - Model loading ~2-3s acceptable (one-time cost)
        - Monitor VRAM: should not exceed 2GB per model (|NFR-18.2|)

        Error Handling
        --------------
        - Raise FileNotFoundError if model file missing
        - Raise RuntimeError if VRAM allocation fails

        References
        ----------
        |TDD-16.6|, |SAD-16.3|
        """
        pass

    def translate(self, text: str, source_lang: str, target_lang: str,
                  num_alternatives: int = 0) -> TranslationResult:
        """
        Translate text from source to target language.

        Parameters
        ----------
        text : str
            Source language text to translate
        source_lang : str
            ISO 639-1 source code
        target_lang : str
            ISO 639-1 target code
        num_alternatives : int
            Number of alternative translations to generate (0-5)

        Returns
        -------
        TranslationResult
            Translation with confidence and alternatives (|ICD-24|)

        Implementation Notes
        --------------------
        1. Ensure model loaded: self.load_model_pair(source_lang, target_lang)
        2. Tokenize input: input_ids = self.tokenizer.encode(text)
        3. Prepare ONNX inputs: {"input_ids": input_ids, "attention_mask": [...]}
        4. Run inference: outputs = session.run(None, inputs)
        5. Decode output: translated_text = self.tokenizer.decode(outputs[0])
        6. Calculate confidence from logits (softmax of top prediction)
        7. If num_alternatives > 0: beam search for N-best list
        8. Construct TranslationResult
        9. Return result

        Performance
        -----------
        - Must complete <1.5s for 100 tokens (|NFR-18.3|)
        - Use batch_size=1 for single translation
        - Consider token-level caching for repeated phrases

        Quality Assurance
        -----------------
        - If confidence <0.7: Set flag in result (|FSD-20.9|)
        - Log translations with confidence <0.5 for review

        References
        ----------
        |TDD-16.3|, |ICD-23|, |ICD-24|, |FSD-20.4|
        """
        pass
~~~

----------
